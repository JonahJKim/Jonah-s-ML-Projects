{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom torch import tensor\nfrom sklearn.model_selection import train_test_split\nimport torchtext\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\nimport spacy\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet34\nimport random\nimport matplotlib.pyplot as plt\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-06T19:36:18.864971Z","iopub.execute_input":"2022-08-06T19:36:18.865567Z","iopub.status.idle":"2022-08-06T19:36:18.873859Z","shell.execute_reply.started":"2022-08-06T19:36:18.865531Z","shell.execute_reply":"2022-08-06T19:36:18.872673Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"# data processing block\n\n''' DATASET LOADING SEGMENT'''\n# load csv\ntrain = pd.read_csv('../input/flickr8k/captions.txt')\n\n# add image path column\ntrain['path'] = '../input/flickr8k/Images/' + train['image']\n\n# remove unnecessary image name column\ntrain.drop(columns=['image'], inplace=True)\n\n# create train/val split \nxtrain, xval, ytrain, yval = train_test_split(train['path'], train['caption'], test_size=0.2)\ntrain_data = list(zip(xtrain, ytrain))\nval_data = list(zip(xval, yval))\n\n# sort based on length\ntrain_data.sort(key=lambda x: len(x[1]))\nval_data.sort(key=lambda x: len(x[1]))\n\n''' TEXT PROCESSING SEGMENT'''\n# text processing\ntokenizer = get_tokenizer('basic_english')\n\n# vocabulary\ndef yield_tokens(iterator):\n    for _, text in iterator:\n        yield tokenizer(text)\nvocab = build_vocab_from_iterator(yield_tokens(train_data), max_tokens=10000, min_freq=2, specials=['<pad>', '<sos>', '<eos>', '<unk>'], special_first=True)\nvocab.set_default_index(vocab['<unk>'])\n\n# text to indices\ndef text_pipeline(text):\n    return vocab(tokenizer(text))\n\n'''IMAGE PROCESSING SEGMENT'''\n# define transforms\ntransform = transforms.Compose([transforms.Resize((224, 224)),\n#                                 transforms.RandomCrop((299, 299)),\n                                transforms.ToTensor(), \n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# collate_fn\ndef collate_fn(batch):\n    image_list, text_list = [], []\n    for image, text in batch:\n        image = transform(Image.open(image))\n        image_list.append(image)\n        text = '<SOS> ' + text + ' <EOS>'\n        text = text_pipeline(text)\n        text_list.append(tensor(text, dtype=torch.int64))\n    image_list = torch.stack(image_list)\n    text_list = torch.nn.utils.rnn.pad_sequence(text_list)\n    return image_list, text_list\n\n# create dataloader from dataset\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=2)\nval_loader = DataLoader(val_data, batch_size=64, shuffle=False, collate_fn=collate_fn, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:18.901605Z","iopub.execute_input":"2022-08-06T19:36:18.901871Z","iopub.status.idle":"2022-08-06T19:36:19.484922Z","shell.execute_reply.started":"2022-08-06T19:36:18.901846Z","shell.execute_reply":"2022-08-06T19:36:19.483859Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"# # def show_image(img, title=None):\n# #     #unnormalize \n# #     img[0] = img[0] * 0.229\n# #     img[1] = img[1] * 0.224 \n# #     img[2] = img[2] * 0.225 \n# #     img[0] += 0.485 \n# #     img[1] += 0.456 \n# #     img[2] += 0.406\n# #     img = img.numpy().transpose((1, 2, 0))\n# #     plt.imshow(img)\n# #     if title is not None:\n# #         plt.title(title)\n# #     plt.pause(0.002)\n    \n# image, label = next(iter(train_loader))\n# image = image.to(device)\n# for i in image:\n#     show_image(i.cpu())\n#     print(model.caption_image(i.unsqueeze(0)))\n# # next(iter(train_loader))[1].shape\n# # model.caption_image(image)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:19.486667Z","iopub.execute_input":"2022-08-06T19:36:19.487037Z","iopub.status.idle":"2022-08-06T19:36:19.491820Z","shell.execute_reply.started":"2022-08-06T19:36:19.487007Z","shell.execute_reply":"2022-08-06T19:36:19.490854Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"# next(iter(train_loader))[0].shape\nprint(len(vocab))","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:19.493393Z","iopub.execute_input":"2022-08-06T19:36:19.494071Z","iopub.status.idle":"2022-08-06T19:36:19.504028Z","shell.execute_reply.started":"2022-08-06T19:36:19.494034Z","shell.execute_reply":"2022-08-06T19:36:19.502673Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"# encoder block\nclass Encoder(nn.Module):\n    # output size is english_vocab_size (embedding done by decoder)\n    def __init__(self, output_size):\n        super().__init__()\n        self.model = resnet34(pretrained=True).to(device)\n        for param in self.model.parameters():\n            param.requires_grad_(False)\n        self.model.fc = nn.Linear(self.model.fc.in_features, output_size).to(device)\n        self.model.fc.parameters(True)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    \n    def forward(self, x):\n        output = self.dropout(self.relu(self.model(x)))\n        return output.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:19.507055Z","iopub.execute_input":"2022-08-06T19:36:19.507400Z","iopub.status.idle":"2022-08-06T19:36:19.515509Z","shell.execute_reply.started":"2022-08-06T19:36:19.507365Z","shell.execute_reply":"2022-08-06T19:36:19.514440Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.dropout = nn.Dropout(0.5)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, image_features, text):\n        embedding = self.dropout(self.embedding(text))\n        image_features = image_features.unsqueeze(0) # makes it (1, N, embed_size)\n        features = torch.cat((image_features, embedding), dim=0) # makes it # (seq_len, N, embed_size)\n        output, _ = self.rnn(features) # (seq_len, N, hidden_size)\n        output = self.fc(output) # (seq_len, N, output_size (vocab size))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:19.517090Z","iopub.execute_input":"2022-08-06T19:36:19.517450Z","iopub.status.idle":"2022-08-06T19:36:19.528454Z","shell.execute_reply.started":"2022-08-06T19:36:19.517416Z","shell.execute_reply":"2022-08-06T19:36:19.527513Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"# CNN2RNN block\n\ntorch. set_printoptions(profile=\"full\")\n\nclass CNN2RNN(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n    \n    def forward(self, image, text):\n        features = self.encoder(image) # input: (N, channels, H, W), output: (N, embed_size)\n        outputs = self.decoder(features, text) # input: (N, embed_size) and (seq_len, N), output: (seq_len, N, vocab_size)\n        return outputs\n    \n    def caption_image(self, image, max_len=50):\n        result = []\n        with torch.no_grad():\n            features = self.encoder(image).unsqueeze(0) # (1, 1, embed_size)\n\n            states = None\n            \n            for _ in range(max_len):\n                output, states = self.decoder.rnn(features, states) # output: (1, 1, hidden_size)\n#                 print(output)\n#                 print(states)\n#                 print(output.squeeze(0)[:, 0:10])\n                output = self.decoder.fc(output.squeeze(0)) # output: (1, output_size)\n#                 print(output[:, 0:10])\n                output = output.argmax(1) # output: (1)\n                \n                result.append(output.item())\n                features = self.decoder.embedding(output).unsqueeze(0)# features: (1, 1, embed_size)\n                print(features)\n                # change features to next\n                if vocab.get_itos()[output.item()] == '<eos>':\n                    break\n                \n        return [vocab.get_itos()[item] for item in result]","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:19.529920Z","iopub.execute_input":"2022-08-06T19:36:19.530926Z","iopub.status.idle":"2022-08-06T19:36:19.541999Z","shell.execute_reply.started":"2022-08-06T19:36:19.530874Z","shell.execute_reply":"2022-08-06T19:36:19.541046Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"# training set-up block\nepochs = 20\nlr=3e-4\nhidden_size = 256\nembedding_size = 256\ninput_size = len(vocab)\noutput_size = len(vocab)\nnum_layers = 1\ndropout_prob = 0\nteacher_force_ratio = 1\n\nencoder = Encoder(hidden_size).to(device)\ndecoder = Decoder(input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob).to(device)\nmodel = CNN2RNN(encoder, decoder).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\nloss_fn = nn.CrossEntropyLoss(ignore_index=vocab.get_stoi()['<pad>'])\n\ndef check_accuracy(iterator, teacher_force_ratio):\n    image, text = next(iter(iterator))\n    image = image.to(device)\n    text = text.to(device)\n    \n    output = model(image, text)\n    output = output.reshape(-1, output.shape[2])\n    text = text.reshape(-1)\n    prediction = output.argmax(1)\n    \n    total = 0\n    correct = 0\n    L = text.shape[0]\n    for i in range(L):\n        if text[i] != 0:\n            total += 1\n        if text[i] == prediction[i]:\n            correct += 1\n            \n    return 100 * correct / total","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:19.544947Z","iopub.execute_input":"2022-08-06T19:36:19.545661Z","iopub.status.idle":"2022-08-06T19:36:20.046114Z","shell.execute_reply.started":"2022-08-06T19:36:19.545634Z","shell.execute_reply":"2022-08-06T19:36:20.044538Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"# training block\ncheck_train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\ncheck_val_loader = DataLoader(val_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\n\nfor epoch in range(epochs):\n    print(f'Epoch: [{epoch} / {epochs}]')\n    for i, (image, text) in enumerate(train_loader):\n        image = image.to(device) # (N, channels, H, W)\n        text = text.to(device) #(seq_len, N)\n        \n        outputs = model(image, text[:-1])\n        \n        \n        if i % 10 == 0:\n            \n            print('True Label: ', end = '')\n            for num in (text[:, 0]):\n                    print(vocab.get_itos()[num.item()] + ' ', end='')\n            print()\n            \n            \n            print('Predicted: ', end='')\n            print(model.caption_image(image[0:1]))\n        \n        \n        outputs = outputs.reshape(-1, outputs.shape[2])\n        text = text.reshape(-1)\n        loss = loss_fn(outputs, text)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        if i % 200 == 0:\n            print(f'Loss: {loss}')\n            \n            train_accuracy = check_accuracy(check_train_loader, 1)\n            print(f'Train Accuracy: {train_accuracy}%')\n    \n            val_accuracy = check_accuracy(check_val_loader, 0)\n            print(f'Validation Accuracy: {val_accuracy}%') \n        \n#         print(text.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:20.048819Z","iopub.execute_input":"2022-08-06T19:36:20.049787Z","iopub.status.idle":"2022-08-06T19:36:51.760014Z","shell.execute_reply.started":"2022-08-06T19:36:20.049747Z","shell.execute_reply":"2022-08-06T19:36:51.756046Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"def show_image(img, title=None):\n    #unnormalize \n    img[0] = img[0] * 0.229\n    img[1] = img[1] * 0.224 \n    img[2] = img[2] * 0.225 \n    img[0] += 0.485 \n    img[1] += 0.456 \n    img[2] += 0.406\n    img = img.numpy().transpose((1, 2, 0))\n    plt.imshow(img)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.002)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:51.763203Z","iopub.status.idle":"2022-08-06T19:36:51.765641Z","shell.execute_reply.started":"2022-08-06T19:36:51.765389Z","shell.execute_reply":"2022-08-06T19:36:51.765415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = DataLoader(train_data, batch_size=64, collate_fn=collate_fn, shuffle=True)\nimage, text = next(iter(temp))\nimage = image.to(device)\ntext = text.to(device)\ntext.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:51.769966Z","iopub.status.idle":"2022-08-06T19:36:51.772987Z","shell.execute_reply.started":"2022-08-06T19:36:51.772610Z","shell.execute_reply":"2022-08-06T19:36:51.772641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(64):\n    print('True Label: ', end = '')\n    for num in (text[:, i]):\n        if num != 0:\n            print(vocab.get_itos()[num.item()] + ' ', end='')\n    print()\n    print('Predicted Label: ', end='')\n    output = model(image, text, 1)\n    output = output.argmax(2)\n    for num in output[:, i]:\n        if num != 0:\n            print(vocab.get_itos()[num.item()] + ' ', end='')\n        \n#     print(output.shape)\n    show_image(image[i].cpu())","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:51.775753Z","iopub.status.idle":"2022-08-06T19:36:51.781271Z","shell.execute_reply.started":"2022-08-06T19:36:51.781012Z","shell.execute_reply":"2022-08-06T19:36:51.781036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch[1].shape","metadata":{"execution":{"iopub.status.busy":"2022-08-06T19:36:51.786746Z","iopub.status.idle":"2022-08-06T19:36:51.787487Z","shell.execute_reply.started":"2022-08-06T19:36:51.787251Z","shell.execute_reply":"2022-08-06T19:36:51.787274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}