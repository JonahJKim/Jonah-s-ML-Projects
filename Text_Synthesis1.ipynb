{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Synthesis1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.10.0\n",
        "# !python -m spacy download en\n",
        "# !python -m spacy download de"
      ],
      "metadata": {
        "id": "pULRoqO-D_a4"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import spacy\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "omQ53zSX6NqV"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data processing block\n",
        "# tokenizer\n",
        "spacy_ger = spacy.load('de_core_news_sm')\n",
        "spacy_eng = spacy.load('en_core_web_sm')\n",
        "\n",
        "def ger_tokenizer(text): return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
        "def eng_tokenizer(text): return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "# field\n",
        "german = Field(tokenize=ger_tokenizer, lower=True, init_token='<sos>', eos_token='<eos>')\n",
        "english = Field(tokenize=eng_tokenizer, lower=True, init_token='<sos>', eos_token='<eos>')\n",
        "\n",
        "# dataset\n",
        "train_data, val_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))\n",
        "\n",
        "# vocabulary\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "\n",
        "# iterator\n",
        "train_iter, val_iter, test_itr = BucketIterator.splits((train_data, val_data, test_data), batch_size=64, sort_within_batch=True, sort_key=lambda x: len(x.src), device=device)"
      ],
      "metadata": {
        "id": "BGrDmRls77dF"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder block\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_prob)\n",
        "    \n",
        "  def forward(self, x, hidden, cell):\n",
        "    x = x.unsqueeze(0)\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    output, (hidden, cell) = self.rnn(embedding, (hidden, cell)) \n",
        "    return hidden, cell\n",
        "\n",
        "  def init_hiddencell(self, batch_size):\n",
        "    hidden = torch.zeros(2, batch_size, 1024).to(device)\n",
        "    cell = torch.zeros(2, batch_size, 1024).to(device)\n",
        "    return hidden, cell"
      ],
      "metadata": {
        "id": "hGT6E2Hl8TxT"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder block\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_prob)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x, hidden, cell):\n",
        "    x = x.unsqueeze(0)\n",
        "    x = self.dropout(self.embedding(x))\n",
        "    output, (hidden, cell) = self.rnn(x, (hidden, cell))\n",
        "    output = self.fc(output)\n",
        "    output = output.squeeze(0)\n",
        "    return output, hidden, cell"
      ],
      "metadata": {
        "id": "by-ZYCn496b-"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Seq2Seq block\n",
        "# class Seq2Seq(nn.Module):\n",
        "#   def __init__(self, encoder, decoder):\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.encoder = encoder\n",
        "#     self.decoder = decoder\n",
        "\n",
        "#   def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
        "#     hidden, cell = self.encoder(source)\n",
        "#     outputs = torch.zeros((len(target), len(target[0]), len(english.vocab))).to(device)\n",
        "#     x = target[0]\n",
        "#     for i in range(1, len(target)):\n",
        "#       output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "#       outputs[i] = output\n",
        "#       best_guess = output.argmax(1)\n",
        "#       x = target[i] if random.random() < teacher_forcing_ratio else best_guess\n",
        "#     return outputs\n",
        "\n",
        "# Seq2Seq block\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
        "    x = source[0]\n",
        "    batch_size = source.shape[1]\n",
        "    hidden, cell = self.encoder.init_hiddencell(batch_size)\n",
        "    for i in range(1, len(source)):\n",
        "      hidden, cell = self.encoder(x, hidden, cell)\n",
        "      x = source[i]\n",
        "\n",
        "\n",
        "\n",
        "    outputs = torch.zeros((len(target), len(target[0]), len(english.vocab))).to(device)\n",
        "    x = target[0]\n",
        "    for i in range(1, len(target)):\n",
        "      output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "      outputs[i] = output\n",
        "      best_guess = output.argmax(1)\n",
        "      x = target[i] if random.random() < teacher_forcing_ratio else best_guess\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "LuD3FSK5AEmD"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train set-up block\n",
        "\n",
        "encoder_input_size = len(german.vocab)\n",
        "decoder_input_size = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "num_layers = 2\n",
        "dropout_prob = 0.5\n",
        "embedding_size = 300\n",
        "hidden_size = 1024\n",
        "\n",
        "encoder = Encoder(encoder_input_size, embedding_size, hidden_size, num_layers, dropout_prob).to(device)\n",
        "decoder = Decoder(decoder_input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob).to(device)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=english.vocab.stoi['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\""
      ],
      "metadata": {
        "id": "7Rwv4Z9FBf1s"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # HELPER\n",
        "# def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "#     spacy_ger = spacy.load(\"de_core_news_sm\")\n",
        "#     if type(sentence) == str: tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "#     else: tokens = [token.lower() for token in sentence]\n",
        "#     tokens.insert(0, german.init_token)\n",
        "#     tokens.append(german.eos_token)\n",
        "#     text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "#     sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "#     with torch.no_grad(): hidden, cell = model.encoder(sentence_tensor)\n",
        "#     outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "#     for _ in range(max_length):\n",
        "#         previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "#         with torch.no_grad():\n",
        "#             output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "#             best_guess = output.argmax(1).item()\n",
        "#         outputs.append(best_guess)\n",
        "#         if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]: break\n",
        "#     translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "#     return translated_sentence[1:]\n",
        "\n",
        "# HELPER\n",
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
        "    if type(sentence) == str: tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else: tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    hidden, cell = model.encoder.init_hiddencell(1)\n",
        "    x = sentence_tensor[0]\n",
        "    with torch.no_grad(): \n",
        "      for i in range(1, len(sentence_tensor)):\n",
        "        hidden, cell = model.encoder(x, hidden, cell)\n",
        "        x = sentence_tensor[i]\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "        outputs.append(best_guess)\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]: break\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]"
      ],
      "metadata": {
        "id": "Kt7MUx9pCXgf"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TEST BLOCK\n",
        "\n",
        "# next(iter(train_iter)).src.shape"
      ],
      "metadata": {
        "id": "aZ2SBrZNOFIT"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training block\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.eval()\n",
        "  print(f'Epoch: [{epoch} / {epochs}]')\n",
        "  training_sentence = translate_sentence(model, sentence, german, english, device)\n",
        "  print(f'Sentence: \\n {training_sentence}')\n",
        "\n",
        "  model.train()\n",
        "  for i, batch in enumerate(train_iter):\n",
        "    xbatch = batch.src.to(device)\n",
        "    ybatch = batch.trg.to(device)\n",
        "\n",
        "    output = model(xbatch, ybatch)\n",
        "\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    ybatch = ybatch[1:].reshape(-1)\n",
        "\n",
        "    loss = loss_fn(output, ybatch)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrQnG3YNCRGy",
        "outputId": "91af9bbc-728f-4bf4-8481-79f07621ca74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0 / 20]\n",
            "Sentence: \n",
            " ['paddle', 'paddle', 'paddle', 'paddle', 'paddle', 'paddle', 'paddle', 'paddle', 'approached', 'rose', 'poncho', 'signs', 'appear', 'appear', 'missed', 'runners', 'runners', 'tongue', 'tongue', 'honor', 'kimonos', 'kimonos', 'weathered', 'calf', 'calf', 'tosses', 'web', 'examine', 'paddle', 'paddle', 'paddle', 'paddle', 'paddle', 'paddle', 'approached', 'paddle', 'paddle', 'paddle', 'approached', 'rose', 'appear', 'rising', 'trio', 'rising', 'tools', 'tools', 'instructors', 'exercises', 'guys', 'paddle']\n",
            "Epoch: [1 / 20]\n",
            "Sentence: \n",
            " ['a', 'couple', 'in', 'a', 'a', 'and', 'a', 'a', 'a', 'a', 'a', 'the', 'background', '.', '<eos>']\n",
            "Epoch: [2 / 20]\n",
            "Sentence: \n",
            " ['a', '<unk>', 'with', 'a', '<unk>', '<unk>', '<unk>', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
            "Epoch: [3 / 20]\n",
            "Sentence: \n",
            " ['a', '<unk>', 'with', 'with', 'a', '<unk>', 'is', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
            "Epoch: [4 / 20]\n",
            "Sentence: \n",
            " ['a', '<unk>', 'with', 'with', 'a', 'number', 'of', 'a', 'by', 'a', 'large', 'of', 'a', '.', '<eos>']\n",
            "Epoch: [5 / 20]\n",
            "Sentence: \n",
            " ['a', 'man', 'with', 'a', 'a', 'a', 'from', 'a', 'large', 'by', 'a', 'large', '.', '.', '<eos>']\n",
            "Epoch: [6 / 20]\n",
            "Sentence: \n",
            " ['a', 'bull', 'with', 'many', '<unk>', 'pulled', 'from', 'a', 'large', 'large', 'large', 'large', 'large', '.', '.', '<eos>']\n",
            "Epoch: [7 / 20]\n",
            "Sentence: \n",
            " ['a', 'boat', 'with', 'a', 'a', 'pulled', 'pulled', 'by', 'a', 'large', 'surrounded', 'by', 'a', 'large', '.', '.', '<eos>']\n",
            "Epoch: [8 / 20]\n",
            "Sentence: \n",
            " ['a', 'bull', 'with', '<unk>', '<unk>', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', '.', '.', '<eos>']\n",
            "Epoch: [9 / 20]\n",
            "Sentence: \n",
            " ['a', 'boat', 'with', 'many', '<unk>', 'pulled', 'pulled', 'by', 'a', 'by', 'a', 'large', '.', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dknRyCuXDDTT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}