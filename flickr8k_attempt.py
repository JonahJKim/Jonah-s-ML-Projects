# -*- coding: utf-8 -*-
"""Flickr8k Attempt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GLIj8r30jb5lrLtUCzw3T7Y1b-uj0EQ2
"""

import numpy as np 
import pandas as pd
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torch.utils.data import DataLoader, Dataset
from PIL import Image
from torch import tensor
from sklearn.model_selection import train_test_split
import torchtext
from torchtext.vocab import build_vocab_from_iterator
from torchtext.data.utils import get_tokenizer
import spacy
import torchvision.transforms as transforms
from torchvision.models import resnet34

# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# data processing block

''' DATASET LOADING SEGMENT'''
# load csv
train = pd.read_csv('../input/flickr8k/captions.txt')

# add image path column
train['path'] = '../input/flickr8k/Images/' + train['image']

# remove unnecessary image name column
train.drop(columns=['image'], inplace=True)

# create train/val split 
xtrain, xval, ytrain, yval = train_test_split(train['path'], train['caption'], test_size=0.2)
train_data = list(zip(xtrain, ytrain))
val_data = list(zip(xval, yval))

# sort based on length
train_data.sort(key=lambda x: len(x[1]))
val_data.sort(key=lambda x: len(x[1]))

''' TEXT PROCESSING SEGMENT'''
# text processing
tokenizer = get_tokenizer('spacy', language='en_core_web_sm')

# vocabulary
def yield_tokens(iterator):
    for _, text in iterator:
        yield tokenizer(text)
vocab = build_vocab_from_iterator(yield_tokens(train_data), max_tokens=10000, min_freq=2, specials=['<unk>'])
vocab.set_default_index(vocab['<unk>'])

# text to indices
def text_pipeline(text):
    return vocab(tokenizer(text))

'''IMAGE PROCESSING SEGMENT'''
# define transforms
transform = transforms.Compose([transforms.Resize(226), 
                                transforms.RandomCrop(224), 
                                transforms.ToTensor(), 
                                transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))])

# collate_fn
def collate_fn(batch):
    image_list, text_list = [], []
    for image, text in batch:
        image = transform(Image.open(image))
        image_list.append(image)
        text = text_pipeline(text)
        text_list.append(tensor(text, dtype=torch.int64))
    image_list = torch.stack(image_list)
    text_list = torch.nn.utils.rnn.pad_sequence(text_list)
    return image_list, text_list

# create dataloader from dataset
train_loader = DataLoader(train_data, batch_size=64, shuffle=False, collate_fn=collate_fn)
val_loader = DataLoader(val_data, batch_size=64, shuffle=False, collate_fn=collate_fn)

# encoder block
class Encoder(nn.Module):
    # output size is english_vocab_size (embedding done by decoder)
    def __init__(self, output_size):
        super().__init__()
        self.model = resnet34(pretrained=True).to(device)
        for param in self.model.parameters():
            param.requires_grad_(False)
        self.model.fc = nn.Linear(self.model.fc.in_features, output_size).to(device)

    
    def forward(self, x):
        output = self.model(x).type(torch.IntTensor)
        output = output.squeeze(1)
        output = output.argmax(1)
        return torch.abs(output).to(device)

class Decoder(nn.Module):
    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob):
        super().__init__()
        self.embedding = nn.Embedding(input_size, embedding_size)
        self.dropout = nn.Dropout(dropout_prob)
        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, dropout=dropout_prob)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x, hidden, cell):
        embedding = self.dropout(self.embedding(x))
        output, (hidden, cell) = self.rnn(embedding, (hidden, cell))
        output = self.fc(output)
        output = output.squeeze(0)
        return output, hidden, cell
    
    def initHidden(self, hidden_size):
        hidden = torch.zeros(1, hidden_size).to(device)
        cell = torch.zeros(1, hidden_size).to(device)
        return hidden, cell

# CNN2RNN block
class CNN2RNN(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, image, text, teacher_force_ratio):
        first_output = self.encoder(image)
        
        outputs = torch.zeros(len(text), len(text[0]), len(vocab)).to(device)
        hidden, cell = self.decoder.initHidden(256)
        x = first_output
        
        for i in range(1, len(text)):
            output, hidden, cell = self.decoder(x, hidden, cell)
            x = text[i].to(device)
            outputs[i] = output
        return outputs

# training set-up block
epochs = 10
lr=0.0001
hidden_size = 256
embedding_size = 256
input_size = len(vocab)
output_size = len(vocab)
num_layers = 1
dropout_prob = 0
teacher_force_ratio = 1

encoder = Encoder(output_size).to(device)
decoder = Decoder(input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob).to(device)
model = CNN2RNN(encoder, decoder).to(device)

optimizer = optim.Adam(model.parameters(), lr=lr)
loss_fn = nn.CrossEntropyLoss()

def check_accuracy(iterator):
    image, text = next(iter(iterator))
    image = image.to(device)
    text = text.to(device)
    
    output = model(image, text, 1)
    output = output.reshape(-1, output.shape[2])
    text = text.reshape(-1)
    prediction = output.argmax(1)
    total = text.shape[0]
    correct = (prediction == text).sum()
    return 100 * correct / total

# training block
check_train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)
check_val_loader = DataLoader(val_data, batch_size=64, shuffle=True, collate_fn=collate_fn)

for epoch in range(epochs):
    print(f'Epoch: [{epoch} / {epochs}]')
    for i, (image, text) in enumerate(train_loader):
        image = image.to(device)
        text = text.to(device)
        
        outputs = model(image, text, 1)
        outputs = outputs.reshape(-1, outputs.shape[2])
        text = text.reshape(-1)
        loss = loss_fn(outputs, text)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        if i % 50 == 0:
            print(f'Loss: {loss}')
            
            train_accuracy = check_accuracy(check_train_loader)
            print(f'Train Accuracy: {train_accuracy}%')
    
            val_accuracy = check_accuracy(check_val_loader)
            print(f'Validation Accuracy: {val_accuracy}%') 
        
#         print(text.shape)
