# -*- coding: utf-8 -*-
"""Transformers w/ new torchtext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jECoQcKwYKBoN43sptMy0Vw7ihyyagwY
"""

# !pip install torchtext==0.10.0
# !python -m spacy download en
# !python -m spacy download de
# !pip install torchdata

# import block
import torch
import torch.nn as nn
import torch.optim as optim
import spacy
from torchtext.datasets import Multi30k
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader
from torchtext.data.utils import get_tokenizer
from torchtext.data.functional import to_map_style_dataset

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# tokenizer
en_tokenizer = get_tokenizer('basic_english')
de_tokenizer = get_tokenizer('basic_english')

# data iterator
train_iter, val_iter = Multi30k(split=('train', 'valid'))
# train_iter = to_map_style_dataset(train_iter)
# val_iter = to_map_style_dataset(val_iter)

# # vocabulary
def german_yield_tokens(iterator):
  for german, _ in iterator:
    yield de_tokenizer(german)

def english_yield_tokens(iterator):
  for _, english in iterator:
    yield en_tokenizer(english)

english_vocab = build_vocab_from_iterator(english_yield_tokens(train_iter), specials=['<unk>'])
english_vocab.set_default_index(english_vocab['<unk>'])

train_iter, val_iter = Multi30k(split=('train', 'valid'))

# train_iter, val_iter = Multi30k(split=('train', 'valid'))
german_vocab = build_vocab_from_iterator(german_yield_tokens(train_iter), specials=['<unk>'])
german_vocab.set_default_index(german_vocab['<unk>'])

# helper functions for collate_fn
def de_text_pipeline(text):
  return german_vocab(de_tokenizer(text))

def en_text_pipeline(text):
  return english_vocab(en_tokenizer(text))

# collate_fn
def collate_fn(batch):
  german_list, english_list = [], []
  for german, english in batch:
    german_list.append(torch.tensor(de_text_pipeline(german), dtype=torch.int64))
    english_list.append(torch.tensor(en_text_pipeline(english), dtype=torch.int64))
  german_list = torch.nn.utils.rnn.pad_sequence(german_list, padding_value=0)
  english_list = torch.nn.utils.rnn.pad_sequence(english_list, padding_value=0)
  return german_list.to(device), english_list.to(device)

# data loader
# train_iter, val_iter = Multi30k(split=('train', 'valid'))
train_iter, val_iter = Multi30k(split=('train', 'valid'))

train_loader = DataLoader(train_iter, batch_size=32, collate_fn=collate_fn)
val_loader = DataLoader(val_iter, batch_size=64, collate_fn=collate_fn)

# IMPORTANT: train_iter actually shrinks when you take out one. so you must recreate the train_iter before data loader or you will be feeding it an empty dataset

test = to_map_style_dataset(train_iter)
test2 = Multi30k(split='train')
dataloadertest = DataLoader(test, batch_size=8, collate_fn=collate_fn, shuffle=True)
dataloadertest2 = DataLoader(test2, batch_size=8, collate_fn=collate_fn)

# 1. use dataloadertest but shuffle=True (never run out)
# 2. use dataloadertest2, always gets one removed, even if you do "for this in that" or next(iter())

i = 0
for batch in dataloadertest2:
  i += 1
  if i % 1000 == 0:
    print(i)
print('done')

# english_vocab.get_stoi()[]

class Transformer(nn.Module):
  def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, embedding_size, max_len, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout):
    super().__init__()
    self.src_embedding = nn.Embedding(src_vocab_size, embedding_size)
    self.src_positional = nn.Embedding(max_len, embedding_size)
    self.trg_embedding = nn.Embedding(trg_vocab_size, embedding_size)
    self.trg_positional = nn.Embedding(max_len, embedding_size)
    self.dropout = nn.Dropout(dropout)
    self.src_padding = src_pad_idx

    self.transformer = nn.Transformer(embedding_size, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout)
    self.fc = nn.Linear(embedding_size, trg_vocab_size)

  def src_mask(self, source):
    source = source.transpose(0, 1) == self.src_padding
    return source 

  def forward(self, source, target):
    source_length, N = source.shape
    target_length, N = target.shape

    # get src/trg word embeddings
    src_embedding = self.src_embedding(source)
    trg_embedding = self.trg_embedding(target)

    # get src/trg positional vectors
    src_position = self.src_positional(torch.arange(0, source_length).unsqueeze(1).expand(source_length, N).to(device))
    trg_position = self.trg_positional(torch.arange(0, target_length).unsqueeze(1).expand(target_length, N).to(device))

    # context vector
    src_context = self.dropout(src_embedding + src_position)
    trg_context = self.dropout(trg_embedding + trg_position)

    src_mask = self.src_mask(source).to(device)
    trg_mask = self.transformer.generate_square_subsequent_mask(target_length).to(device)

    out = self.transformer(src_context, trg_context, src_key_padding_mask=src_mask, tgt_mask=trg_mask)
    out = self.fc(out)
    return out

# training set-up block
epochs = 10
lr = 3e-4
src_vocab_size = len(german_vocab)
trg_vocab_size = len(english_vocab)
src_pad_idx = 0
embedding_size = 512
max_len = 100
num_heads = 8
num_encoder_layers = 3
num_decoder_layers = 3
forward_expansion = 2048
dropout = 0.1

model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, embedding_size, max_len, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout).to(device)

optimizer = optim.Adam(model.parameters(), lr=lr)
loss_fn = nn.CrossEntropyLoss(ignore_index=src_pad_idx)

sentence = "ein pferd geht unter einer br√ºcke neben einem boot."

def translate_sentence(model, sentence, german, english, device, max_length=50):
    # Load german tokenizer
    spacy_ger = spacy.load("de_core_news_sm")

    # Create tokens using spacy and everything in lower case (which is what our vocab is)
    if type(sentence) == str:
        tokens = [token.text.lower() for token in spacy_ger(sentence)]
    else:
        tokens = [token.lower() for token in sentence]

    # Add <SOS> and <EOS> in beginning and end respectively
    tokens.insert(0, german.init_token)
    tokens.append(german.eos_token)

    # Go through each german token and convert to an index
    text_to_indices = [german.vocab.stoi[token] for token in tokens]

    # Convert to Tensor
    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)

    outputs = [english.vocab.stoi["<sos>"]]
    for i in range(max_length):
        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)

        with torch.no_grad():
            output = model(sentence_tensor, trg_tensor)

        best_guess = output.argmax(2)[-1, :].item()
        outputs.append(best_guess)

        if best_guess == english.vocab.stoi["<eos>"]:
            break

    translated_sentence = [english.vocab.itos[idx] for idx in outputs]
    # remove start token
    return translated_sentence[1:]

# training block
train_iter, val_iter = Multi30k(split=('train', 'valid'))
train_loader = DataLoader(train_iter, batch_size=32, collate_fn=collate_fn)
val_loader = DataLoader(val_iter, batch_size=64, collate_fn=collate_fn)

for epoch in range(epochs):
  print(f'Epoch [{epoch} / {epochs}]')
  # model.eval()
  # translated_sentence = translate_sentence(model, sentence, german, english, device, max_length = 100)
  # print(f'Translated: \n {translated_sentence}')


  model.train()
  
  for i, batch in enumerate(train_loader):
    model.train()
    source = batch[0]
    target = batch[1]

    output = model(source, target[:-1])
    output = output.reshape(-1, output.shape[2])
    target = target[1:].reshape(-1)

    loss = loss_fn(output, target)
    loss.backward()

    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)

    optimizer.step()
    optimizer.zero_grad()

    model.eval()
    if i % 100 == 0:
      with torch.no_grad():
        print(f'Loss: {loss}')

        val_batch = next(iter(val_loader))
        val_source = val_batch[0]
        val_target = val_batch[1]

        output = model(val_source, val_target[:-1])
        output = output.reshape(-1, output.shape[2])
        val_target = val_target[1:].reshape(-1)

        prediction = output.argmax(1)
        total = val_target.shape[0]
        correct = (prediction == val_target).sum()
        accuracy = 100 * correct / total
        print(f'Accuracy: {accuracy}%')

