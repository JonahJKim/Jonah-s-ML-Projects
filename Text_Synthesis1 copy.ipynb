{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Synthesis1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# imports block\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import spacy\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Q7CE70Z1VBRD"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data processing block\n",
        "\n",
        "# Tokenizer\n",
        "spacy_ger = spacy.load('de_core_news_sm')\n",
        "spacy_eng = spacy.load('en_core_web_sm')\n",
        "\n",
        "def eng_tokenizer(text): return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "def ger_tokenizer(text): return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
        "\n",
        "# Field\n",
        "english = Field(lower=True, tokenize=eng_tokenizer, init_token='<sos>', eos_token='<eos>')\n",
        "german = Field(lower=True, tokenize=ger_tokenizer, init_token='<sos>', eos_token='<eos>')\n",
        "\n",
        "# Dataset\n",
        "train_data, val_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))\n",
        "\n",
        "# Vocabulary\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "\n",
        "# DataLoader\n",
        "train_iterator, val_iterator, test_iterator = BucketIterator.splits((train_data, val_data, test_data), batch_size=64, sort_within_batch=True, sort_key=lambda x: len(x.src), device=device)"
      ],
      "metadata": {
        "id": "GN9S6RGKV7eY"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder block\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_prob):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    # actual input is a tensor of arbitrary shape containing indices, output is embedding_vector\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size) # input_size is german_dict size (BUT NOT ACTUAL ONE-HOT VECTOR), output is embedding size).\n",
        "\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    output, (hidden, cell) = self.rnn(embedding)\n",
        "    return hidden, cell"
      ],
      "metadata": {
        "id": "T3hsA8dtWgbH"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder block\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers \n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_prob)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input, hidden, cell):\n",
        "      input = input.unsqueeze(0)\n",
        "      embedding = self.dropout(self.embedding(input))\n",
        "      output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "      output = self.fc(output)\n",
        "      output = output.squeeze(0)\n",
        "      return output, hidden, cell"
      ],
      "metadata": {
        "id": "m3R-gD8OZNk0"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "    seq_length = target.shape[0]\n",
        "    batch_size = target.shape[1]\n",
        "    vocab_size = len(english.vocab)\n",
        "    outputs = torch.zeros(seq_length, batch_size, vocab_size).to(device)\n",
        "    hidden, cell = self.encoder(source)\n",
        "\n",
        "    # grab start token\n",
        "    x = target[0] \n",
        "\n",
        "    for i in range(1, seq_length):\n",
        "      output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "      outputs[i] = output\n",
        "\n",
        "      best_guess = outputs.argmax(1)\n",
        "\n",
        "      x = target[i] if random.random() < teacher_force_ratio else best_guess\n",
        "    \n",
        "    return outputs"
      ],
      "metadata": {
        "id": "b9U5EcInZph-"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training set-up block\n",
        "\n",
        "encoder_input_size = len(german.vocab)\n",
        "decoder_input_size = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "dropout_prob = 0.5\n",
        "lr = 0.001\n",
        "epochs = 20\n",
        "\n",
        "encoder = Encoder(encoder_input_size, embedding_size, hidden_size, num_layers, dropout_prob).to(device)\n",
        "decoder = Decoder(decoder_input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob).to(device)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=english.vocab.stoi['<pad>'])\n",
        "\n",
        "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\""
      ],
      "metadata": {
        "id": "v6bJQhZRkWx6"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper\n",
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
        "    if type(sentence) == str: tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else: tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "    with torch.no_grad(): hidden, cell = model.encoder(sentence_tensor)\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "        outputs.append(best_guess)\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]: break\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]"
      ],
      "metadata": {
        "id": "OLmeO20ionD-"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training block\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f'Epoch [{epoch} / {epochs}')\n",
        "  model.eval()\n",
        "  translated_sentence = translate_sentence(model, sentence, german, english, device, max_length=50)\n",
        "  print(f'Translated: \\n {translated_sentence}')\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for i, batch in enumerate(train_iterator):\n",
        "    xbatch = batch.src.to(device)\n",
        "    ybatch = batch.trg.to(device)\n",
        "\n",
        "    outputs = model(xbatch, ybatch)\n",
        "\n",
        "    outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
        "    ybatch = ybatch[1:].reshape(-1)\n",
        "\n",
        "    loss = loss_fn(outputs, ybatch)\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "hnq_1ylepPch",
        "outputId": "54b10864-e10e-4fcb-f2ff-0c19715702c7"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-31f12e9b629d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch [{epoch} / {num_epochs}]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S_6b0L2FqG0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}