# -*- coding: utf-8 -*-
"""NLP with attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14bKV8rAXsK9jhXhtfnKhBTd-l8OqXTKn
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchtext
from torchtext.legacy.datasets import Multi30k
from torchtext.legacy.data import Field, BucketIterator
import spacy
import random
from torch import tensor

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# data processing block

spacy_ger = spacy.load('de_core_news_sm')
spacy_eng = spacy.load('en_core_web_sm')

# tokenizers for field
def ger_tokenizer(text): return [tok.text for tok in spacy_ger.tokenizer(text)]
def eng_tokenizer(text): return [tok.text for tok in spacy_eng.tokenizer(text)]

# fields
german = Field(tokenize=ger_tokenizer, lower=True, init_token='<sos>', eos_token='<eos>')
english = Field(tokenize=eng_tokenizer, lower=True, init_token='<sos>', eos_token='<eos>')

# datasets
train_data, val_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))

# vocabulary
german.build_vocab(train_data, max_size=10000, min_freq=2)
english.build_vocab(train_data, max_size=10000, min_freq=2)

# iterators
train_iter, val_itr, test_itr = BucketIterator.splits((train_data, val_data, test_data), batch_size=64, sort_within_batch=True, sort_key=lambda x: len(x.src), device=device)

from unicodedata import bidirectional
# encoding block
class Encoder(nn.Module):
  # input size: german_vocab size, embedding_size: 300, hidden_size: 1024, num_layers: 2, dropout_prob: 0.5
  def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_prob):
    super().__init__()

    self.dropout = nn.Dropout(dropout_prob)
    self.embedding = nn.Embedding(input_size, embedding_size)
    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_prob, bidirectional=True)#, bidirectional=True)
    self.combine = nn.Linear(hidden_size * 2, hidden_size)

  def forward(self, x):
    embedding = self.dropout(self.embedding(x))
    encoder_states, (hidden, cell) = self.rnn(embedding)
    hidden = self.combine(torch.cat((hidden[0:1], hidden[1:2]), dim=2))
    cell = self.combine(torch.cat((cell[0:1], cell[1:2]), dim=2))


    return encoder_states, hidden, cell

# decoder block
class Decoder(nn.Module):
  # input_size: english_vocab size, embedding_size: 300, hidden_size: 1024, num_layers:2, dropout_prob:0.5
  def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob):
    super().__init__()
    self.dropout = nn.Dropout(dropout_prob)
    self.embedding = nn.Embedding(input_size, embedding_size)
    self.rnn = nn.LSTM(hidden_size * 2 + embedding_size, hidden_size, num_layers, dropout=dropout_prob)
    self.fc = nn.Linear(hidden_size, output_size)

    self.energy = nn.Linear(hidden_size * 3, 1)

  def forward(self, x, encoder_states, hidden, cell):
    x = x.unsqueeze(0)
    embedding = self.dropout(self.embedding(x))

    sequence_length = encoder_states.shape[0]
    hidden_repeat = hidden.repeat(sequence_length, 1, 1)
    energy = F.relu(self.energy(torch.cat((hidden_repeat, encoder_states), dim=2)))
    activations = F.softmax(energy, dim=0) # (seq_length, N, 1), encoder_states: (seq_length, N, hidden_size * 2)

    activations = activations.permute(1, 2, 0) # (N, 1, seq_length)
    encoder_states = encoder_states.permute(1, 0, 2) # (N, seq_length, hidden_size * 2)

    context_vector = torch.bmm(activations, encoder_states) # (N, 1, hidden_size * 2)
    context_vector = context_vector.permute(1, 0, 2)
    rnn_input = torch.cat((context_vector, embedding), dim=2)
    output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))
    output = self.fc(output)
    output = output.squeeze(0)

    return output, hidden, cell

# seq2seq block
class Seq2Seq(nn.Module):
  def __init__(self, encoder, decoder):
    super().__init__()

    self.encoder = encoder
    self.decoder = decoder
  
  def forward(self, source, target, teacher_force_ratio=0.5):
    encoder_states, hidden, cell = self.encoder(source)

    outputs = torch.zeros(len(target), len(target[0]), len(english.vocab)).to(device)
    # <sos> token
    x = target[0]
    for i in range(1, len(target)):
      output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)
      outputs[i] = output
      best_guess = output.argmax(1)
      x = target[i] if random.random() < teacher_force_ratio else best_guess
    return outputs

# training set-up block
encoder_input_size = len(german.vocab)
decoder_input_size = len(english.vocab)
output_size = len(english.vocab)

embedding_size = 300
hidden_size = 1024
num_layers = 1
epochs = 20
lr = 0.001
dropout_prob = 0.5

encoder = Encoder(encoder_input_size, embedding_size, hidden_size, num_layers, dropout_prob).to(device)
decoder = Decoder(decoder_input_size, embedding_size, hidden_size, output_size, num_layers, dropout_prob).to(device)
model = Seq2Seq(encoder, decoder).to(device)

loss_fn = nn.CrossEntropyLoss(ignore_index=english.vocab.stoi['<pad>'])
optimizer = optim.Adam(model.parameters(), lr=lr)

sentence = "ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen."

# HELPER
def translate_sentence(model, sentence, german, english, device, max_length=50):
    spacy_ger = spacy.load("de_core_news_sm")
    if type(sentence) == str: tokens = [token.text.lower() for token in spacy_ger(sentence)]
    else: tokens = [token.lower() for token in sentence]
    tokens.insert(0, german.init_token)
    tokens.append(german.eos_token)
    text_to_indices = [german.vocab.stoi[token] for token in tokens]
    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)
    with torch.no_grad(): encoder_states, hidden, cell = model.encoder(sentence_tensor)
    outputs = [english.vocab.stoi["<sos>"]]
    for _ in range(max_length):
        previous_word = torch.LongTensor([outputs[-1]]).to(device)
        with torch.no_grad():
            output, hidden, cell = model.decoder(previous_word, encoder_states, hidden, cell)
            best_guess = output.argmax(1).item()
        outputs.append(best_guess)
        if output.argmax(1).item() == english.vocab.stoi["<eos>"]: break
    translated_sentence = [english.vocab.itos[idx] for idx in outputs]
    return translated_sentence[1:]

# training block
for epoch in range(epochs):
  print(f'Epoch: [{epoch} / {epochs}]')
  training_sentence = translate_sentence(model, sentence, german, english, device)
  print(f'Sentence: \n {training_sentence}')

  for i, batch in enumerate(train_iter):
    xbatch = batch.src.to(device)
    ybatch = batch.trg.to(device)
    outputs = model(xbatch, ybatch)

    outputs = outputs[1:].reshape(-1, outputs.shape[2])
    ybatch = ybatch[1:].reshape(-1)

    loss = loss_fn(outputs, ybatch)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()





